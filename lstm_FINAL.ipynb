{"cells":[{"cell_type":"code","execution_count":1,"id":"f41ffdb0-c340-4870-96c7-1faf0cd489b2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"f41ffdb0-c340-4870-96c7-1faf0cd489b2","executionInfo":{"status":"error","timestamp":1743625514316,"user_tz":-330,"elapsed":666,"user":{"displayName":"Upal Pahari","userId":"17652444065368943002"}},"outputId":"707a9682-d79e-4fd1-e453-e5948e3ba840"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'mediapipe'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b002cc26de07>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import cv2\n","import mediapipe as mp\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","# Initialize MediaPipe Pose\n","class PoseDetector():\n","    def __init__(self, detectionCon=0.5, trackCon=0.5):\n","        self.mpPose = mp.solutions.pose\n","        self.pose = self.mpPose.Pose(\n","            min_detection_confidence=detectionCon, min_tracking_confidence=trackCon)\n","\n","    def findPose(self, img):\n","        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        self.results = self.pose.process(imgRGB)\n","        return img\n","\n","    def findPosition(self, img):\n","        lmList = []\n","        if self.results.pose_landmarks:\n","            h, w, _ = img.shape\n","\n","            left_hip = self.results.pose_landmarks.landmark[23]\n","            right_hip = self.results.pose_landmarks.landmark[24]\n","            mid_hip_x = (left_hip.x + right_hip.x) / 2\n","            mid_hip_y = (left_hip.y + right_hip.y) / 2\n","\n","            left_shoulder = self.results.pose_landmarks.landmark[11]\n","            right_shoulder = self.results.pose_landmarks.landmark[12]\n","            shoulder_width = abs(left_shoulder.x - right_shoulder.x) + 1e-6\n","\n","            for lm in self.results.pose_landmarks.landmark:\n","                cx = (lm.x - mid_hip_x) / shoulder_width\n","                cy = (lm.y - mid_hip_y) / shoulder_width\n","                cz = lm.z / shoulder_width\n","                lmList.append([cx, cy, cz])\n","\n","        return np.array(lmList).flatten() if lmList else np.zeros(99)\n","\n","def extract_landmarks(video_path, label, max_frames=1000):\n","    if not os.path.exists(video_path):\n","        print(f\"⚠️ ERROR: Video file '{video_path}' not found.\")\n","        return []\n","\n","    cap = cv2.VideoCapture(video_path)\n","    detector = PoseDetector()\n","    all_landmarks = []\n","\n","    frame_count = 0\n","\n","    while cap.isOpened():\n","        success, img = cap.read()\n","        if not success or frame_count >= max_frames:\n","            break\n","\n","        img = detector.findPose(img)\n","        lm_data = detector.findPosition(img)\n","\n","        if lm_data is not None and np.any(lm_data):\n","            all_landmarks.append(np.append(lm_data, label))\n","            frame_count += 1\n","\n","    cap.release()\n","\n","    print(f\"✅ Extracted {frame_count} frames from {video_path}\")\n","    return all_landmarks\n","\n","# Define Activities (Danger = 0, Non-Danger = 1)\n","activities = {\n","    \"fighting.mp4\": 0,\n","    \"Kick1.mp4\": 0,\n","    \"Punch1.mp4\": 0,\n","    \"Shove1.mp4\": 0,\n","    \"slap1.mp4\": 0,\n","    \"Nun1.mp4\": 0,\n","    \"Shoot1.mp4\": 0,\n","    \"sitting.mp4\": 1,\n","    \"walking3.mp4\": 1,\n","    \"jumping.mp4\": 1,\n","    \"bowling1.mp4\": 1\n","}\n","\n","# Extract Data\n","all_data = []\n","for video, label in activities.items():\n","    all_data.extend(extract_landmarks(video, label))\n","\n","# Save Dataset\n","if all_data:\n","    df = pd.DataFrame(all_data)\n","    df.to_csv(\"pose_dataset.csv\", index=False)\n","    print(\"✅ Dataset saved as 'pose_dataset.csv' 🚀\")\n","else:\n","    print(\"⚠️ No data extracted. Check video files.\")"]},{"cell_type":"code","execution_count":null,"id":"927d63fe-2dd2-4e81-a319-8fb570599530","metadata":{"id":"927d63fe-2dd2-4e81-a319-8fb570599530","outputId":"d4859f21-cf6c-4e62-a8c7-bec2d0d6bbd2"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\KIIT\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 149ms/step - loss: 0.1596 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.0039 - val_sparse_categorical_accuracy: 0.9969\n","Epoch 2/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 146ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9992 - val_loss: 0.0029 - val_sparse_categorical_accuracy: 1.0000\n","Epoch 3/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 161ms/step - loss: 0.0010 - sparse_categorical_accuracy: 0.9993 - val_loss: 0.0036 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 4/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 162ms/step - loss: 8.7181e-04 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.0019 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 5/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 129ms/step - loss: 7.0311e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0038 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 6/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 145ms/step - loss: 8.8804e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0015 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 7/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 2.3302e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0059 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 8/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 0.0011 - sparse_categorical_accuracy: 0.9991 - val_loss: 0.0049 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 9/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - loss: 1.9827e-04 - sparse_categorical_accuracy: 0.9999 - val_loss: 0.0026 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 10/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 138ms/step - loss: 1.0738e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0041 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 11/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 3.2914e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0052 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 12/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 8.4182e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0047 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 13/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 3.7477e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0048 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 14/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 175ms/step - loss: 1.4304e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0048 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 15/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 158ms/step - loss: 3.3474e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0058 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 16/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - loss: 1.5538e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0055 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 17/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 160ms/step - loss: 9.7959e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0054 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 18/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 165ms/step - loss: 1.0284e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0055 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 19/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 171ms/step - loss: 7.5824e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0053 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 20/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 1.6541e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0059 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 21/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 136ms/step - loss: 1.9799e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0058 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 22/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 1.2217e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 23/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - loss: 9.9516e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0072 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 24/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - loss: 5.0093e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0070 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 25/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 183ms/step - loss: 3.1897e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 26/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 182ms/step - loss: 3.4022e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 27/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 167ms/step - loss: 2.8394e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0070 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 28/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 169ms/step - loss: 1.0368e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 29/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 169ms/step - loss: 4.1816e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0070 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 30/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 167ms/step - loss: 4.1075e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0072 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 31/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 166ms/step - loss: 2.4737e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0072 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 32/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 166ms/step - loss: 3.6836e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0071 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 33/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 161ms/step - loss: 3.0884e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0074 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 34/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 164ms/step - loss: 2.8234e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 35/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 173ms/step - loss: 2.2997e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0071 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 36/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 165ms/step - loss: 7.4762e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0072 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 37/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 3.1004e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0072 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 38/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 160ms/step - loss: 2.8348e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0071 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 39/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - loss: 5.2210e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 40/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 172ms/step - loss: 2.1459e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 41/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 171ms/step - loss: 2.4102e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0067 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 42/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 163ms/step - loss: 1.9039e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 43/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 174ms/step - loss: 2.4990e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 44/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 175ms/step - loss: 1.7583e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0075 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 45/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 163ms/step - loss: 1.4855e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0076 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 46/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - loss: 2.8828e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0071 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 47/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 175ms/step - loss: 2.2898e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0064 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 48/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 172ms/step - loss: 1.7173e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0063 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 49/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 166ms/step - loss: 3.2238e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n","Epoch 50/50\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 164ms/step - loss: 9.4745e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"name":"stdout","output_type":"stream","text":["✅ Model trained & saved as 'lstm_activity_model.h5'\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n","\n","# Load Dataset\n","csv_file = \"pose_dataset.csv\"\n","df = pd.read_csv(csv_file).values\n","X = df[:, :-1]\n","y = df[:, -1]\n","\n","# Prepare Sequences for LSTM\n","TIME_STEPS = 30\n","FEATURES = 99\n","\n","X_seq, y_seq = [], []\n","for i in range(len(X) - TIME_STEPS):\n","    X_seq.append(X[i:i+TIME_STEPS])\n","    y_seq.append(y[i+TIME_STEPS])\n","\n","X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n","\n","# Split Train & Test\n","X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n","\n","# Define LSTM Model\n","model = Sequential([\n","    Bidirectional(LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, FEATURES))),\n","    Dropout(0.3),\n","    Bidirectional(LSTM(128)),\n","    Dense(64, activation=\"relu\"),\n","    Dropout(0.3),\n","    Dense(2, activation=\"softmax\")  # 2 Classes: Danger (0) & Non-Danger (1)\n","])\n","\n","# Compile Model\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n","\n","# Train Model\n","model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n","\n","# Save Model\n","model.save(\"lstm_activity_model.h5\")\n","print(\"✅ Model trained & saved as 'lstm_activity_model.h5'\")"]},{"cell_type":"code","execution_count":null,"id":"2374e3ec-b98c-43eb-b0f9-02a1e58cef0e","metadata":{"id":"2374e3ec-b98c-43eb-b0f9-02a1e58cef0e","outputId":"36acd9c5-d643-4891-ab78-7955f2b12546"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"]}],"source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","\n","# Load trained model\n","model = tf.keras.models.load_model(\"lstm_activity_model.h5\")\n","\n","# Labels\n","actions = [\"DANGER\", \"SAFE\"]\n","colors = [(0, 0, 255), (0, 255, 0)]  # RED (Danger) & GREEN (Non-Danger)\n","\n","# Load Test Video\n","cap = cv2.VideoCapture(\"slap1 (1).mp4\")\n","detector = PoseDetector()\n","sequence = []\n","\n","cv2.namedWindow(\"Activity Recognition\", cv2.WINDOW_NORMAL)\n","\n","while cap.isOpened():\n","    success, img = cap.read()\n","    if not success:\n","        break\n","\n","    img = detector.findPose(img)\n","    lm_data = detector.findPosition(img)\n","\n","    if lm_data is not None and np.any(lm_data):\n","        sequence.append(lm_data)\n","        if len(sequence) > TIME_STEPS:\n","            sequence.pop(0)\n","\n","        if len(sequence) == TIME_STEPS:\n","            input_seq = np.expand_dims(np.array(sequence), axis=0)\n","            prediction = np.argmax(model.predict(input_seq), axis=1)[0]\n","            label = actions[prediction]\n","            color = colors[prediction]\n","\n","            cv2.rectangle(img, (20, 30), (250, 100), color, -1)  # Display box\n","            cv2.putText(img, label, (50, 80),\n","                        cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 3)\n","\n","    img_resized = cv2.resize(img, (800, 600))\n","    cv2.imshow(\"Activity Recognition\", img_resized)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":2,"id":"188122d4-eab6-4217-a34f-fcc236d8a197","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"id":"188122d4-eab6-4217-a34f-fcc236d8a197","executionInfo":{"status":"error","timestamp":1743587741288,"user_tz":-330,"elapsed":283,"user":{"displayName":"Upal Pahari","userId":"17652444065368943002"}},"outputId":"c129cb1b-555d-473a-82b0-7299b31485b1"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] Unable to synchronously open file (unable to open file: name = 'lstm_activity_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6f2c10d4bec3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_activity_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'lstm_activity_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}],"source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","\n","# Load trained model\n","model = tf.keras.models.load_model(\"lstm_activity_model.h5\")\n","\n","# Labels\n","actions = [\"DANGER\", \"SAFE\"]\n","colors = [(0, 0, 255), (0, 255, 0)]  # RED (Danger) & GREEN (Non-Danger)\n","\n","# Load Test Video\n","cap = cv2.VideoCapture(\"test_video.mp4\")\n","detector = PoseDetector()\n","sequence = []\n","\n","cv2.namedWindow(\"Activity Recognition\", cv2.WINDOW_NORMAL)\n","\n","while cap.isOpened():\n","    success, img = cap.read()\n","    if not success:\n","        break\n","\n","    img = detector.findPose(img)\n","    lm_data = detector.findPosition(img)\n","\n","    if lm_data is not None and np.any(lm_data):\n","        sequence.append(lm_data)\n","        if len(sequence) > TIME_STEPS:\n","            sequence.pop(0)\n","\n","        if len(sequence) == TIME_STEPS:\n","            input_seq = np.expand_dims(np.array(sequence), axis=0)\n","            prediction = np.argmax(model.predict(input_seq), axis=1)[0]\n","            label = actions[prediction]\n","            color = colors[prediction]\n","\n","            # Adjusted smaller rectangle & font\n","            cv2.rectangle(img, (20, 20), (180, 60), color, -1)  # Smaller box\n","            cv2.putText(img, label, (30, 50),\n","                        cv2.FONT_HERSHEY_COMPLEX, 0.8, (255, 255, 255), 2)  # Smaller font\n","\n","    img_resized = cv2.resize(img, (800, 600))\n","    cv2.imshow(\"Activity Recognition\", img_resized)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"id":"ce957837-931f-4b7e-8305-9fc6ecd73eef","metadata":{"id":"ce957837-931f-4b7e-8305-9fc6ecd73eef"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}